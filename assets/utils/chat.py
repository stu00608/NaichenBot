import openai
from collections import deque
from transformers import GPT2TokenizerFast
from asgiref.sync import sync_to_async

tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")

class User:
    def __init__(self, id):
        self.id = id
        self.history = Conversation(5)
        self.count = 0

    # These user objects should be accessible by ID, for example if we had a bunch of user
    # objects in a list, and we did `if 1203910293001 in user_list`, it would return True
    # if the user with that ID was in the list
    def __eq__(self, other):
        return self.id == other.id

    def __hash__(self):
        return hash(self.id)

    def __repr__(self):
        return f"User(id={self.id}, history={self.history})"

    def __str__(self):
        return self.__repr__()

class Conversation:
    def __init__(self, limit) -> None:
        with open("assets/texts/intro.txt") as f:
            self.intro = f.read()
        with open("assets/texts/conversation.txt") as f:
            self.prior_conv = f.read()
        
        self.conv = deque(maxlen=limit)
    
    def render(self):
        active_conv = ""
        for p, c in self.conv:
            active_conv += f"人類: {p}\n後藤一里: {c}\n"
        return self.intro + self.prior_conv + active_conv
    
    def prepare_prompt(self, prompt):
        return self.render() + f"人類: {prompt}\n後藤一里: "
    
    def append_conversation(self, prompt, message):
        self.conv.append((prompt, message))
    
    def __len__(self):
        return get_token_len(self.render())
    
    def __repr__(self) -> str:
        return self.render()
    
    def __str__(self) -> str:
        return self.render()

def get_token_len(text):
    return len(tokenizer(text)["input_ids"])
        
async def generate_conversation(prompt):
    """
    Requests a completion from the OpenAI Text-DaVinci-002 model and returns the completion as a string.

    Parameters:
    - prompt (str): The prompt for which to generate a completion.

    Returns:
    - completion (str): The completion generated by the model.
    """
    completions = await sync_to_async(openai.Completion.create)(
        engine="text-davinci-003",
        prompt=prompt,
        max_tokens=150,
        temperature=0.9,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0.6,
        stop=["\n", " 人類:", " 後藤一里:"]
    )
    message = completions.choices[0].text
    return message